---
phase: 01-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - data/ecdysis/download.py
  - data/ecdysis/occurrences.py
autonomous: true
requirements:
  - PIPE-01
  - PIPE-02
  - PIPE-03

must_haves:
  truths:
    - "Running `python data/ecdysis/download.py --db 164` completes without error and writes a zip file to disk"
    - "Running `python data/ecdysis/occurrences.py <zip>` completes without hanging and writes ecdysis.parquet"
    - "The output Parquet contains all 10 required columns: scientificName, family, genus, specificEpithet, year, month, recordedBy, fieldNumber, latitude, longitude"
    - "Records with null decimalLatitude or decimalLongitude are excluded from the output (46318 rows in → ~45754 rows out)"
  artifacts:
    - path: "data/ecdysis/download.py"
      provides: "CLI entry point for Ecdysis zip download"
      contains: "args.db"
    - path: "data/ecdysis/occurrences.py"
      provides: "CLI entry point for Parquet output"
      contains: "sys.argv[1]"
    - path: "data/ecdysis.parquet"
      provides: "Output Parquet with all required columns"
  key_links:
    - from: "download.py __main__ block"
      to: "make_dump()"
      via: "make_dump({'db': args.db})"
      pattern: "make_dump\\(\\{'db': args\\.db\\}\\)"
    - from: "occurrences.py __main__ block"
      to: "to_parquet()"
      via: "to_parquet(df, Path('ecdysis.parquet'))"
      pattern: "to_parquet\\(df"
    - from: "to_parquet()"
      to: "null-coord filter"
      via: "df[df['ecdysis_decimalLatitude'].notna() & ...]"
      pattern: "notna\\(\\)"
---

<objective>
Fix all six identified bugs in the Ecdysis data pipeline so that both scripts run end-to-end and produce correct output.

Purpose: The pipeline is the foundation for all downstream map work. Until the scripts run cleanly, no Parquet file can be generated and no frontend feature can be verified against real data.
Output: Two corrected Python scripts and a verified ecdysis.parquet containing all fields required by PIPE-03.
</objective>

<execution_context>
@/Users/rainhead/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rainhead/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-pipeline/01-RESEARCH.md
@data/ecdysis/download.py
@data/ecdysis/occurrences.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix download.py — add --db argument, fix parse_args, call make_dump</name>
  <files>data/ecdysis/download.py</files>
  <action>
Make three changes to the `__main__` block of `data/ecdysis/download.py`:

1. Add `--db` argument to argparse (required):
   ```python
   parser.add_argument('-d', '--db', required=True, help='Ecdysis database ID (e.g. 164)')
   ```

2. Fix `parse_args` call — remove `sys.argv` argument so argparse defaults to `sys.argv[1:]`:
   ```python
   args = parser.parse_args()   # was: parser.parse_args(sys.argv)
   ```

3. Add `make_dump` call after `parse_args`, passing the db argument:
   ```python
   make_dump({'db': args.db})
   ```

The complete corrected `__main__` block should be:
```python
if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(prog='download.py', description='Download archives from Ecdysis')
    parser.add_argument('-d', '--db', required=True, help='Ecdysis database ID (e.g. 164)')
    parser.add_argument('-s', '--state')
    args = parser.parse_args()
    make_dump({'db': args.db})
```

Do NOT change any other part of the file. The `download_params`, `make_dump`, and `download_url` definitions are correct as-is.
  </action>
  <verify>
```bash
cd /Users/rainhead/dev/beeatlas/data && uv run python ecdysis/download.py --help
```
Must print usage showing `--db` as a required option with no error. Do NOT actually run the download (it makes a live HTTP request) — `--help` is sufficient to confirm argparse is wired correctly.
  </verify>
  <done>
`python ecdysis/download.py --help` exits 0 and shows `-d/--db` in the usage output. `python ecdysis/download.py` (no args) exits non-zero with "error: the following arguments are required: -d/--db".
  </done>
</task>

<task type="auto">
  <name>Task 2: Fix occurrences.py — remove pdb, fix __main__ block, expand to_parquet columns</name>
  <files>data/ecdysis/occurrences.py</files>
  <action>
Make three changes to `data/ecdysis/occurrences.py`:

**Change 1: Remove pdb.set_trace() (line 95)**

Delete this line entirely:
```python
    import pdb; pdb.set_trace()
```

**Change 2: Replace the entire `to_parquet` function body** with the version that:
- Filters null-coordinate rows using `.notna()` on source columns (NOT on geometry)
- Selects all 11 required columns (ecdysis_id plus 10 semantic columns)
- Renames to clean output names per PIPE-03
- Converts to plain DataFrame before writing (avoids GeoParquet format)

```python
def to_parquet(df: pd.DataFrame, out: Path | IO[bytes]):
    # Filter records with null coordinates before output
    df = df[df['ecdysis_decimalLatitude'].notna() & df['ecdysis_decimalLongitude'].notna()]
    # Select required columns and rename for output
    df = df[[
        'ecdysis_id',
        'ecdysis_decimalLongitude',
        'ecdysis_decimalLatitude',
        'ecdysis_scientificName',
        'ecdysis_family',
        'ecdysis_genus',
        'ecdysis_specificEpithet',
        'ecdysis_year',
        'ecdysis_month',
        'ecdysis_recordedBy',
        'ecdysis_fieldNumber',
    ]].rename(columns={
        'ecdysis_decimalLatitude': 'latitude',
        'ecdysis_decimalLongitude': 'longitude',
        'ecdysis_scientificName': 'scientificName',
        'ecdysis_family': 'family',
        'ecdysis_genus': 'genus',
        'ecdysis_specificEpithet': 'specificEpithet',
        'ecdysis_year': 'year',
        'ecdysis_month': 'month',
        'ecdysis_recordedBy': 'recordedBy',
        'ecdysis_fieldNumber': 'fieldNumber',
    })
    # Convert to plain DataFrame to avoid writing GeoParquet (geometry column breaks hyparquet)
    pd.DataFrame(df).to_parquet(out, engine='pyarrow', index=False)
```

Note: the filter uses `df['ecdysis_decimalLatitude']` (with `ecdysis_` prefix) because `read_occurrences()` calls `add_prefix('ecdysis_')` before returning. Do NOT filter on `df['decimalLatitude']` — that column does not exist after the prefix is added.

**Change 3: Fix the `__main__` block** (currently broken — `Path(zip)` references Python builtin `zip`, not a CLI arg):

Replace:
```python
if __name__ == '__main__':
    df = from_zipfile(Path(zip))
    print(f"Loaded {len(df)} occurrences")
```

With:
```python
if __name__ == '__main__':
    zip_path = Path(sys.argv[1])
    df = from_zipfile(zip_path)
    print(f"Loaded {len(df)} occurrences")
    to_parquet(df, Path("ecdysis.parquet"))
```
  </action>
  <verify>
```bash
cd /Users/rainhead/dev/beeatlas/data && uv run python ecdysis/occurrences.py ecdysis_2026-02-16_1.zip
```

Then verify the output:
```bash
cd /Users/rainhead/dev/beeatlas/data && uv run python -c "
import pyarrow.parquet as pq
f = pq.read_table('ecdysis.parquet')
print('Columns:', f.schema.names)
required = ['scientificName','family','genus','specificEpithet','year','month','recordedBy','fieldNumber','latitude','longitude','ecdysis_id']
missing = [c for c in required if c not in f.schema.names]
print('Missing:', missing or 'none')
print('Rows:', f.num_rows)
assert f.num_rows > 0, 'Output is empty'
assert not missing, f'Missing columns: {missing}'
print('PASS')
"
```

The script must complete without hanging (no pdb prompt). Row count must be less than 46318 (null coords excluded). All required columns must be present.
  </verify>
  <done>
`python ecdysis/occurrences.py ecdysis_2026-02-16_1.zip` runs to completion without hanging, prints "Loaded N occurrences", writes ecdysis.parquet. The Parquet verification script prints "Missing: none" and "PASS". Row count is between 45000 and 46318 (564 null-coord rows excluded per research).
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. `python data/ecdysis/download.py --help` exits 0 and shows `--db` argument
2. `python data/ecdysis/download.py` (no args) exits non-zero with required-arg error
3. `python data/ecdysis/occurrences.py ecdysis_2026-02-16_1.zip` completes without hanging
4. The output ecdysis.parquet contains all 11 columns: ecdysis_id, latitude, longitude, scientificName, family, genus, specificEpithet, year, month, recordedBy, fieldNumber
5. Row count is ~45754 (46318 source rows minus ~564 null-coord rows)
6. No `pdb` import remains in occurrences.py
</verification>

<success_criteria>
Phase 1 is complete when:
- [ ] `download.py --db 164` is wired end-to-end with correct argparse (PIPE-01)
- [ ] `occurrences.py <zip>` runs without hanging and writes Parquet (PIPE-02)
- [ ] Output Parquet contains all PIPE-03 required columns with correct names (PIPE-03)
- [ ] Null-coord rows are excluded without crashing the pipeline
</success_criteria>

<output>
After completion, create `.planning/phases/01-pipeline/01-01-SUMMARY.md` using the summary template at `@/Users/rainhead/.claude/get-shit-done/templates/summary.md`.
</output>
